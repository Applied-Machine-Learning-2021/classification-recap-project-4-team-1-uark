{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "copyright"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/applied-machine-learning-intensive/blob/master/content/04_classification/08_video_processing_project/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khlO4Bu21oZ4"
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlzIlBsScJJ_"
      },
      "source": [
        "# Video Classification with Pre-Trained Models Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTirVS4FWaPx"
      },
      "source": [
        "In this project we will import a pre-existing model that recognizes objects and use the model to identify those objects in a video. We'll edit the video to draw boxes around the identified object, and then we'll reassemble the video so the boxes are shown around objects in the video."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTVUYxPwcHhp"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdIOgOHP1ces"
      },
      "source": [
        "## Exercise 1: Coding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTEOK1ZmqN8"
      },
      "source": [
        "You will process a video frame by frame, identify objects in each frame, and draw a bounding box with a label around each car in the video.\n",
        " \n",
        "Use the [SSD MobileNet V1 Coco](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md) (*ssd_mobilenet_v1_coco*) model. The video you'll process can be found [on Pixabay](https://pixabay.com/videos/cars-motorway-speed-motion-traffic-1900/). The 640x360 version of the video is smallest and easiest to handle, though any size should work since you must scale down the images for processing.\n",
        " \n",
        "Your program should:\n",
        " \n",
        "* Read in a video file (use the one in this colab if you want)\n",
        "* Load the TensorFlow model linked above\n",
        "* Loop over each frame of the video\n",
        "* Scale the frame down to a size the model expects\n",
        "* Feed the frame to the model\n",
        "* Loop over detections made by the model\n",
        "* If the detection score is above some threshold, draw a bounding box onto the frame and put a label in or near the box\n",
        "* Write the frame back to a new video\n",
        " \n",
        "Some tips:\n",
        " \n",
        "* Processing an entire video is slow, so consider truncating the video or skipping over frames during development. Skipping frames will make the video choppy. But you'll be able to see a wider variety of images than you would with a truncated video with all of the original frames in the clip.\n",
        "* The model expects a 300x300 image. You'll likely have to scale your frames to fit the model. When you get a bounding box, that box is relative to the scaled image. You'll need to scale the bounding box out to the original image size.\n",
        "* Don't start by trying to process the video. Instead, capture one frame and work with it until you are happy with your object detection, bounding boxes, and labels. Once you get those done, use the same logic on the other frames of the video.\n",
        "* The [Coco labels file](https://github.com/nightrome/cocostuff/blob/master/labels.txt) can be used to identify classified objects.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgoZ0IOtAeT6"
      },
      "source": [
        "### **Team 1**\n",
        "\n",
        "Alejandra Barroso\n",
        "\n",
        "A'Darius Lee\n",
        "\n",
        "Sam Lefforge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XM35vYWSbim"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnD8d_zXjuKz"
      },
      "source": [
        "# imports\n",
        "import urllib.request\n",
        "import os\n",
        "import tarfile\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe0NGEWIikPG"
      },
      "source": [
        "# Loading model\n",
        "\n",
        "base_url = 'http://download.tensorflow.org/models/object_detection/'\n",
        "file_name = 'ssd_mobilenet_v1_coco_2018_01_28.tar.gz'\n",
        "\n",
        "url = base_url + file_name\n",
        "\n",
        "urllib.request.urlretrieve(url, file_name)\n",
        "\n",
        "dir_name = file_name[0:-len('.tar.gz')]\n",
        "\n",
        "if os.path.exists(dir_name):\n",
        "  shutil.rmtree(dir_name) \n",
        "\n",
        "tarfile.open(file_name, 'r:gz').extractall('./')\n",
        "\n",
        "frozen_graph = os.path.join(dir_name, 'frozen_inference_graph.pb')\n",
        "\n",
        "with tf.io.gfile.GFile(frozen_graph, \"rb\") as f:\n",
        "  graph_def = tf.compat.v1.GraphDef()\n",
        "  loaded = graph_def.ParseFromString(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPTkwoaUmDXS"
      },
      "source": [
        "# Helper function\n",
        "\n",
        "def wrap_graph(graph_def, inputs, outputs, print_graph=False):\n",
        "  wrapped = tf.compat.v1.wrap_function(\n",
        "    lambda: tf.compat.v1.import_graph_def(graph_def, name=\"\"), [])\n",
        "\n",
        "  return wrapped.prune(\n",
        "    tf.nest.map_structure(wrapped.graph.as_graph_element, inputs),\n",
        "    tf.nest.map_structure(wrapped.graph.as_graph_element, outputs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahb5YVi-mED4"
      },
      "source": [
        "# Object detection dictionary\n",
        "dict = {\n",
        "    0:\"background\",\n",
        "    1:\"person\",\n",
        "    2:\"bicycle\",\n",
        "    3:\"car\",\n",
        "    4:\"motorcycle\",\n",
        "    5:\"airplane\",\n",
        "    6:\"bus\",\n",
        "    7:\"train\",\n",
        "    8:\"truck\",\n",
        "    9:\"boat\",\n",
        "    10:\"trafficlight\",\n",
        "    11:\"firehydrant\",\n",
        "    12:\"unknown\",\n",
        "    13:\"stopsign\",\n",
        "    14:\"parkingmeter\",\n",
        "    15:\"bench\",\n",
        "    16:\"bird\",\n",
        "    17:\"cat\",\n",
        "    18:\"dog\",\n",
        "    19:\"horse\",\n",
        "    20:\"sheep\",\n",
        "    21:\"cow\",\n",
        "    22:\"elephant\",\n",
        "    23:\"bear\",\n",
        "    24:\"zebra\",\n",
        "    25:\"giraffe\",\n",
        "    26:\"unknown\",\n",
        "    27:\"backpack\",\n",
        "    28:\"umbrella\",\n",
        "    29:\"unknown\",\n",
        "    30:\"unknown\",\n",
        "    31:\"handbag\",\n",
        "    32:\"tie\",\n",
        "    33:\"suitcase\",\n",
        "    34:\"frisbee\",\n",
        "    35:\"skis\",\n",
        "    36:\"snowboard\",\n",
        "    37:\"sportsball\",\n",
        "    38:\"kite\",\n",
        "    39:\"baseballbat\",\n",
        "    40:\"baseballglove\",\n",
        "    41:\"skateboard\",\n",
        "    42:\"surfboard\",\n",
        "    43:\"tennisracket\",\n",
        "    44:\"bottle\",\n",
        "    45:\"unknown\",\n",
        "    46:\"wineglass\",\n",
        "    47:\"cup\",\n",
        "    48:\"fork\",\n",
        "    49:\"knife\",\n",
        "    50:\"spoon\",\n",
        "    51:\"bowl\",\n",
        "    52:\"banana\",\n",
        "    53:\"apple\",\n",
        "    54:\"sandwich\",\n",
        "    55:\"orange\",\n",
        "    56:\"broccoli\",\n",
        "    57:\"carrot\",\n",
        "    58:\"hotdog\",\n",
        "    59:\"pizza\",\n",
        "    60:\"donut\",\n",
        "    61:\"cake\",\n",
        "    62:\"chair\",\n",
        "    63:\"couch\",\n",
        "    64:\"pottedplant\",\n",
        "    65:\"bed\",\n",
        "    66:\"unknown\",\n",
        "    67:\"diningtable\",\n",
        "    68:\"unknown\",\n",
        "    69:\"unknown\",\n",
        "    70:\"toilet\",\n",
        "    71:\"unknown\",\n",
        "    72:\"tv\",\n",
        "    73:\"laptop\",\n",
        "    74:\"mouse\",\n",
        "    75:\"remote\",\n",
        "    76:\"keyboard\",\n",
        "    77:\"cellphone\",\n",
        "    78:\"microwave\",\n",
        "    79:\"oven\",\n",
        "    80:\"toaster\",\n",
        "    81:\"sink\",\n",
        "    82:\"refrigerator\",\n",
        "    83:\"unknown\",\n",
        "    84:\"book\",\n",
        "    85:\"clock\",\n",
        "    86:\"vase\",\n",
        "    87:\"scissors\",\n",
        "    88:\"teddybear\",\n",
        "    89:\"hairdrier\",\n",
        "    90:\"toothbrush\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGhTiqZniUzd"
      },
      "source": [
        "# Draw the boxes by calling this function\n",
        "\n",
        "def drawBoxes(frame):\n",
        "  image = frame\n",
        "\n",
        "  outputs = (\n",
        "    'num_detections:0',\n",
        "    'detection_classes:0',\n",
        "    'detection_scores:0',\n",
        "    'detection_boxes:0',\n",
        "  )\n",
        "  input_images = [image]\n",
        "      \n",
        "  model = wrap_graph(graph_def=graph_def,\n",
        "                    inputs=[\"image_tensor:0\"],\n",
        "                    outputs=outputs)\n",
        "\n",
        "  tensor = tf.convert_to_tensor(input_images, dtype=tf.uint8)\n",
        "\n",
        "  detections = model(tensor)\n",
        "  boxes = []\n",
        "  i = 0\n",
        "  while detections[3][0][i].numpy().any():\n",
        "    boxes.append((detections[3][0][i].numpy(), detections[1][0][i].numpy()))\n",
        "    i += 1\n",
        "\n",
        "  height = image.shape[0]\n",
        "  width = image.shape[1]\n",
        "  for box in boxes:\n",
        "    label = box[1]\n",
        "    y1 = int(box[0][0] * height)\n",
        "    x1 = int(box[0][1] * width)\n",
        "    y2 = int(box[0][2] * height)  \n",
        "    x2 = int(box[0][3] * width)\n",
        "    image = cv.rectangle(image, (x1, y1), (x2, y2), (0,255,0), 2)\n",
        "    cv.putText(image, dict[label], (x1,y2), cv.FONT_HERSHEY_SIMPLEX, .8, [0,0,0], 2)\n",
        "\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivTzfzQN5jDk"
      },
      "source": [
        "# Generate video\n",
        "\n",
        "input_video = cv.VideoCapture('cars.mp4')\n",
        "\n",
        "height = int(input_video.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
        "width = int(input_video.get(cv.CAP_PROP_FRAME_WIDTH))\n",
        "fps = input_video.get(cv.CAP_PROP_FPS)\n",
        "total_frames = int(input_video.get(cv.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "fourcc = cv.VideoWriter_fourcc(*'mp4v')\n",
        "output_video = cv.VideoWriter('cars-detected.mp4', fourcc, fps, (width, height))\n",
        "\n",
        "for i in range(0, total_frames, 25):\n",
        "  print(i, \" / \", total_frames)\n",
        "  input_video.set(cv.CAP_PROP_POS_FRAMES, i)\n",
        "  ret, frame = input_video.read()\n",
        "\n",
        "  frame = drawBoxes(frame)\n",
        "  if not ret:\n",
        "    raise Exception(\"Problem reading frame\", i, \" from video\")\n",
        "  output_video.write(frame)\n",
        "\n",
        "input_video.release()\n",
        "output_video.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEGDiC-IhcrM"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HniKdSXg0YHR"
      },
      "source": [
        "## Exercise 2: Ethical Implications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4FvC1Aa0ZT5"
      },
      "source": [
        "Even the most basic models have the potential to affect segments of the population in different ways. It is important to consider how your model might positively and negatively affect different types of users.\n",
        "\n",
        "In this section of the project, you will reflect on the positive and negative implications of your model. Frame the context of your model creation using this narrative:\n",
        "\n",
        "> The city of Seattle is attempting to reduce traffic congestion in its downtown area. As part of this project, they plan to allow each local driver one free trip to downtown Seattle per week. After that, the driver will have to pay a $50 toll for each extra day per week driven. As an early proof of concept for this project, your team is tasked with using machine learning to correctly identify automobiles on the road. The next phase of the project will involve detecting license plate numbers and then cross-referencing that data with RFID chips that should be mounted in all local drivers' cars."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkyzwVQr0brd"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy4I2vG60ebd"
      },
      "source": [
        "**Positive Impact**\n",
        "\n",
        "Your model is trying to solve a problem. Think about who will benefit from that problem being solved and write a brief narrative about how the model will help."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k59MK1Ah0fWy"
      },
      "source": [
        "> *People who can afford to pay the $50 will benefit because they will be able to go downtown with less traffic. Regular pedetrians and such in the city will also benifit from the lack of traffic.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzqkrLnk0hMU"
      },
      "source": [
        "**Negative Impact**\n",
        "\n",
        "Models rarely benefit everyone equally. Think about who might be negatively impacted by the predictions your model is making. This person(s) might not be directly using the model, but they might be impacted indirectly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hefa1JdP0kj3"
      },
      "source": [
        "> *People of lower economic status will be negatively impacted because they won't be able to afford the fee and effectively be barred from driving downtown at all, aside from their one free day a week.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uax2HAzd0mHX"
      },
      "source": [
        "**Bias**\n",
        "\n",
        "Models can be biased for many reasons. The bias can come from the data used to build the model (e.g., sampling, data collection methods, available sources) and/or from the interpretation of the predictions generated by the model.\n",
        "\n",
        "Think of at least two ways bias might have been introduced to your model and explain both below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bJGm-qs0oQV"
      },
      "source": [
        "> *One source of bias in the model could be the lighting that the training images were photographed in. For example if all of the images were taken in the daytime outside, then the model will likely have lots of trouble recongizing objects during the night time.*\n",
        "\n",
        "> *Another source of bias in the model could be the choice of objects that it was trained on. For example, the model will recongize common utensils like forks, knives, and spoons, but not chopsticks which is another common utensil in Asian culture.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybb1zAkC0p2e"
      },
      "source": [
        "**Changing the Dataset to Mitigate Bias**\n",
        "\n",
        "Having bias in your dataset is one of the primary ways in which bias is introduced to a machine learning model. Look back at the input data you fed to your model. Think about how you might change something about the data to reduce bias in your model.\n",
        "\n",
        "What change or changes could you make to reduce the bias in your dataset? Consider the data you have, how and where it was collected, and what other sources of data might be used to reduce bias.\n",
        "\n",
        "Write a summary of changes that could be made to your input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFsnF4_h08DD"
      },
      "source": [
        "> *Since the data has potential bias with the choice of objects, increasing the number of classes that are in the dataset could help to mitigate that bias.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChEJbhXA02pW"
      },
      "source": [
        "**Changing the Model to Mitigate Bias**\n",
        "\n",
        "Is there any way to reduce bias by changing the model itself? This could include modifying algorithmic choices, tweaking hyperparameters, etc.\n",
        "\n",
        "Write a brief summary of changes you could make to help reduce bias in your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEAhgO_U0p8Y"
      },
      "source": [
        "> *Since the model has potential bias with dealing with a lack of diversity of lighting, can adjust by training using the output of edge detection run on the dataset. By looking at only the edges, hopefully the bias that comes from certian lighting conditions are mitigated.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rShB5BQv0wix"
      },
      "source": [
        "**Mitigating Bias Downstream**\n",
        "\n",
        "Models make predictions. Downstream processes make decisions. What processes and/or rules should be in place for people and systems interpreting and acting on the results of your model to reduce bias? Describe these rules and/or processes below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C__BwBP-00HN"
      },
      "source": [
        "> *Since the model is only trained to detect certian objects, people and systems downstream should keep that in mind and not use it for anything that it isn't equipped for. For example, if someone in another culture were to use this model, it likely would not detect items that aren't also commonplace in American culture, so they would have to be extra careful to make sure the classes they want are avaliable.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L_4RNXphYtI"
      },
      "source": [
        "---"
      ]
    }
  ]
}